{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T06:47:27.874268400Z",
     "start_time": "2023-11-22T06:47:27.055439400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:31:05.901314Z",
     "start_time": "2023-11-22T08:31:05.897316200Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 定义一维卷积层\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        # 定义池化层\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(16 * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入的十维向量转换为二维张量\n",
    "        x = x.unsqueeze(1)\n",
    "        # 卷积层\n",
    "        x = self.conv1(x)\n",
    "        # 激活函数\n",
    "        x = torch.relu(x)\n",
    "        # 池化层\n",
    "        x = self.pool(x)\n",
    "        # 将张量展平为一维向量\n",
    "        x = x.view(-1, 16 * 4)\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # 激活函数\n",
    "        x = torch.relu(x)\n",
    "        # 输出层\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        print(outputs)\n",
    "        print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "# 定义测试函数\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_acc = correct / total\n",
    "    return test_loss, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "#随机交叉验证数据导入\n",
    "def load_data(cols,data,train_data,train_label,train_sample,test_data,test_label,test_sample):\n",
    "    random.shuffle(cols)\n",
    "    for col in cols:\n",
    "        if cols.index(col) <= int(len(cols)*0.6):\n",
    "            train_data.append(list(data.loc[:,col]))\n",
    "            train_sample.append(col)\n",
    "            if col.split('_')[0][-1] == 'N':\n",
    "                train_label.append(0)\n",
    "            else:\n",
    "                train_label.append(1)\n",
    "        else:\n",
    "            test_data.append(list(data.loc[:,col]))\n",
    "            test_sample.append(col)\n",
    "            if col.split('_')[0][-1] == 'N':\n",
    "                test_label.append(0)\n",
    "            else:\n",
    "                test_label.append(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:31:06.707224300Z",
     "start_time": "2023-11-22T08:31:06.704224300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0638297872340423, 0.0], [1.0411865356599306, 1.573791278814635, 0.6869736571839339, 0.0628330651695134, 0.2184964568142127, 0.1961518491892313, 0.150621055994887, 0.0046293270592335, 0.0501737359213992, 8.133073625525057], [0.1454691880891765, 0.108009132546634, 2.124227158433873, 0.6840327968944395, 0.2620830539986735, 1.5208410793300535, 1.5730070130200686, 0.259539115534229, 0.0026742376969855, 22.035251571307573], [0.0479396831622759, 0.8526415076719087, 0.2353402627966275, 0.0, 0.0, 0.1451264953912536, 0.125514806824868, 0.2745636399293988, 0.385696541805584, 2.706413022161217], [0.0483767102353003, 0.1120090721414266, 0.6994952343807448, 1.432255390991527, 0.0266876185354606, 2.134720174156602, 1.9945072181257224, 0.1772370753853568, 0.1076991081050299, 4.195358694385392], [2.5200613745789258, 0.5674503095183493, 0.8730004761820759, 0.1513200825382265, 0.0268615531132946, 0.4481402444401322, 0.7973404349129626, 0.0, 1.3968007618913214, 5.587203047565286], [0.0, 0.0, 0.0, 0.0833333333333331, 0.0, 0.3611111111111101, 0.277777777777777, 0.0, 6.287878787878773, 0.0], [0.0013914684144588, 0.0816192398145566, 0.3141589707907539, 1.39046389128219, 0.6076862853117448, 0.3865470806664813, 0.4243341473621587, 0.0062377847272361, 0.0189922892596228, 0.1646711768490832], [0.1886312420996878, 0.4568718362227327, 0.2722333565108338, 0.7676304805533042, 1.0930235728796065, 0.5825269900999006, 0.6640012543117005, 0.0470366886171216, 1.0051954451276124, 0.9409382796842464], [0.0972389491056696, 4.841453040469517, 5.914645665572553, 0.1273309404050665, 1.1858442025277969, 0.4924952049600802, 0.4777755475266531, 0.0, 0.2709755118426344, 1.327445470359967], [0.0707463742483196, 1.2645914396887132, 0.0, 0.1111728738187879, 0.0, 0.3705762460626266, 0.3705762460626266, 0.0, 2.9507133592736645, 1.5564202334630317], [0.0201024165220703, 0.0272264307983128, 0.8779042701764791, 0.3663657853847879, 0.2200685598205597, 0.2186659250436814, 0.235594275799109, 0.0, 0.0338567015108553, 0.9426975326928784], [0.0412541254125411, 0.2045517051705166, 0.0, 0.1100110011001097, 0.0, 0.0618811881188117, 0.048129812981298, 0.0, 0.0412541254125411, 5.754950495049494], [0.0, 0.0, 0.0, 3.0817610062893106, 0.0, 1.6142557651991625, 1.3626834381551372, 0.0, 5.450733752620549, 0.0], [0.8183963836474459, 1.402328963596625, 10.492552852392905, 0.6681885621998762, 0.503249950041602, 2.168776256308799, 2.038255996867656, 2.9908033575287605, 0.050793429076723, 4.853725260639693], [0.1190476190476198, 0.0, 0.0, 0.0, 0.0, 0.1851851851851864, 0.1851851851851864, 0.0, 0.748971193415643, 0.0], [0.0275334726459318, 0.2209922453151757, 0.0273788748472523, 0.7102564063403558, 2.121559844124266, 0.4869077745953562, 0.4962378986433133, 0.0113513222931293, 0.0516358315041289, 28.637970711073585], [6.139162198786306, 2.156901603080829, 4.087374064690978, 0.1704387640875002, 0.2651269663583337, 0.2146265918138892, 0.2146265918138892, 0.0, 0.7511930713486122, 2.6512696635833377], [1.4332519957786494, 0.3254163978971941, 1.025839600759349, 0.6300704718429311, 0.2616918155483408, 1.2049524767857578, 1.2722917119715156, 0.2794442121223522, 0.1226411541232103, 18.1299276101869], [0.0, 1.0912698412698414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.109126984126983, 1.190476190476191], [0.0, 0.0, 0.0, 0.1016260162601631, 0.609756097560979, 0.1016260162601631, 0.1016260162601631, 0.0, 2.5711382113821286, 1.829268292682937], [1.5113871635610785, 1.2939958592132517, 0.0, 0.1229296066252589, 0.0194099378881987, 0.5383022774327128, 0.5383022774327128, 0.0, 3.127007924609128, 4.658385093167707], [0.3629265352690051, 1.489603983140222, 2.640575431195049, 0.6364452485139634, 1.0944898613478482, 1.102798689413157, 1.0088938008713468, 0.0981545356222171, 0.1181945778237427, 2.031425335148471], [1.1004214369397172, 1.6515082117839943, 5.575877797490524, 0.3130199209878163, 0.7675675194189235, 1.3346230360251832, 1.3823551636265723, 3.613020441850628, 0.0186494660182787, 3.6017827650361016], [0.4071328605770797, 0.0, 0.2209767170895334, 0.0650876875790989, 0.0482131019104436, 0.1341931336507348, 0.1221398581731239, 0.0273207577492514, 0.2792657582008068, 0.1084794792984982], [0.0702772416022048, 1.200011122450229, 0.8340112980815704, 2.9142517716000254, 0.0548440446500096, 12.872332079199206, 12.447789939819296, 9.1229646504024, 0.0108042204783471, 0.3777736831944456], [0.0, 0.0, 1.5189873417721542, 0.0, 0.0, 0.0, 0.0, 0.0, 7.130801687763723, 0.0], [1.980553034112332, 2.183790328134429, 0.0769262228501253, 0.0745451730952405, 0.0, 0.3610953532119575, 0.2925028045039291, 0.0651687490167823, 0.0578822920713748, 2.9231964683047647], [0.1361173073722133, 0.1990734193926896, 0.431522754803572, 0.2492167634689106, 0.2353183959188392, 0.2544035054920597, 0.2489116609969607, 0.0732245932679864, 0.5857967461438912, 0.5034190787174064], [1.7383686410729855, 0.0017767951974006, 0.0769114677424652, 0.1186916281700889, 0.4784635855311277, 0.3603402889027607, 0.3669463326202155, 5.610860036733058, 0.059024018229581, 6.647781512590247], [0.0185127175471151, 0.7623233079983873, 0.9802169334014446, 17.255962020406983, 5.939691433226623, 1.1891295313818848, 1.3314553378437988, 0.0244732621980606, 0.0853708337481056, 0.1933765946052276], [0.0337944497392647, 0.707663856626454, 1.896690100793499, 0.7628774432137095, 0.0237668417860382, 1.5019117353571008, 1.428096759506144, 0.15699254415438, 0.0275559425995117, 0.9905676352711804], [3.581916449563531, 0.2521008403361359, 1.0918003565062455, 0.0, 0.0, 0.0, 0.0, 0.0, 3.03030303030305, 1.7379679144385138], [0.0, 0.0, 0.0, 0.0, 0.0, 1.744186046511625, 1.744186046511625, 2.536997885835091, 2.382302892796366, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "#导入数据集\n",
    "N_data = pd.read_csv(\"./data/N.txt\",sep=\"\\t\")\n",
    "T_data = pd.read_csv(\"./data/T.txt\",sep=\"\\t\")\n",
    "n_cols = N_data.columns.tolist()[1:]\n",
    "t_cols = T_data.columns.tolist()[1:]\n",
    "t_copy_cols = copy.deepcopy(t_cols)\n",
    "#random.shuffle(t_copy_cols)\n",
    "t_copy_cols = t_copy_cols[:len(n_cols)]\n",
    "data = N_data.merge(T_data)\n",
    "train_data = []\n",
    "train_label = []\n",
    "train_sample = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "test_sample = []\n",
    "load_data(n_cols,data,train_data,train_label,train_sample,test_data,test_label,test_sample)\n",
    "load_data(t_copy_cols,data,train_data,train_label,train_sample,test_data,test_label,test_sample)\n",
    "print(train_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:31:07.451993900Z",
     "start_time": "2023-11-22T08:31:07.443912800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1.])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "# 加载自己的数据集，假设已经将数据存储在名为 \"data.npy\" 的文件中\n",
    "data = torch.from_numpy(np.array(train_data)).float()\n",
    "#data = torch.LongTensor(data)\n",
    "# 加载自己的标签，假设已经将标签存储在名为 \"labels.npy\" 的文件中\n",
    "labels = torch.from_numpy(np.array(train_label)).float()\n",
    "#labels = torch.LongTensor(labels)\n",
    "# 创建 TensorDataset 实例\n",
    "train_dataset = TensorDataset(data, labels)\n",
    "# 加载自己的数据集，假设已经将数据存储在名为 \"data.npy\" 的文件中\n",
    "data = torch.from_numpy(np.array(test_data)).float()\n",
    "#data = torch.LongTensor(data)\n",
    "# 加载自己的标签，假设已经将标签存储在名为 \"labels.npy\" 的文件中\n",
    "labels = torch.from_numpy(np.array(test_label)).float()\n",
    "#labels = torch.LongTensor(labels)\n",
    "test_dataset = TensorDataset(data, labels)\n",
    "labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:31:08.262945200Z",
     "start_time": "2023-11-22T08:31:08.256913600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4208, -0.0152,  0.0315, -0.2340,  0.0633,  0.0652, -0.1410,  0.1994,\n",
      "         -0.1013,  0.1021]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 34\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# 训练模型\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 34\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m train(model, train_loader, criterion, optimizer, device)\n\u001B[0;32m     35\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m test(model, test_loader, criterion, device)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[64], line 46\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, criterion, optimizer, device)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels)\n\u001B[1;32m---> 46\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     48\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     49\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcross_entropy(\u001B[38;5;28minput\u001B[39m, target, weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[0;32m   1180\u001B[0m                            ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction,\n\u001B[0;32m   1181\u001B[0m                            label_smoothing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_smoothing)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3052\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\u001B[38;5;28minput\u001B[39m, target, weight, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "# 定义超参数\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # 数据预处理\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "#\n",
    "# # 加载数据集\n",
    "# train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# 创建模型实例和优化器\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loader\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:31:09.146408900Z",
     "start_time": "2023-11-22T08:31:09.089409400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
